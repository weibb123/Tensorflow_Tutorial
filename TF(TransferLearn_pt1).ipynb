{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "What is it? We will take exist model which has learned from another problem and using them for our own problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 14 22:33:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.13       Driver Version: 462.13       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 307... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8    14W /  N/A |    151MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1680    C+G   C:\\Games\\Steamm\\steam.exe       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Are we using a GPU?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Hub\n",
    "There are tons of existing model created by people, researchers.\n",
    "https://tfhub.dev/\n",
    "\n",
    "We can get better results with only 10% of the data...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\16178\\anaconda3\\envs\\ml\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data (10% of labels)\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_curry'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\fried_rice'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\sushi'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\train'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_curry'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_wings'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\fried_rice'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\grilled_salmon'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\hamburger'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ice_cream'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\pizza'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ramen'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\steak'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\sushi'.\n"
     ]
    }
   ],
   "source": [
    "# How many images in each folder?\n",
    "import os\n",
    "\n",
    "# Walk through 10 percent data directory and list number of files\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loaders (preparing the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:\n",
      "Found 750 images belonging to 10 classes.\n",
      "Testing images:\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = '10_food_classes_10_percent/test/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "print(\"Training images:\")\n",
    "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
    "target_size=IMAGE_SHAPE,\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='categorical')\n",
    "\n",
    "print(\"Testing images:\")\n",
    "test_data= train_datagen.flow_from_directory(test_dir,\n",
    "target_size=IMAGE_SHAPE,\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up callbacks (things to run whilst our model trains)\n",
    "\n",
    "Callbacks -> extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks include:\n",
    "\n",
    "<b>Experiment tracking with TensorBoard</b> - log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.\\\n",
    "<b>Model checkpointing</b> - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.\\\n",
    "<b>Early stopping</b> - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take.\n",
    "\n",
    "We'll see how the TensorBoard callback can be used.\n",
    "\n",
    "The TensorBoard callback can be accessed using tf.keras.callbacks.TensorBoard()\\\n",
    "\n",
    "We will track our modelling experiments using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback\n",
    "import datetime\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you're likely to run multiple experiments. It's a good idea to be able to track them in some way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model using TensorFlow Hub\n",
    "\n",
    "We will be using two models from TensorFlow Hub:\\\n",
    "ResNetV2\\\n",
    "EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Transfer Learning\n",
    "\n",
    "<b>Transfer Learning </b> -> Take a pretrained model as it it and apply it to your task without any changes\\\n",
    "\n",
    "<b>Feature extraction transfer learning </b> -> Take the weight of pretrained model has learned and adjust its outputs to be more suited to your problem.\\\n",
    "\n",
    "<b>Fine-tuning transfer Learning</b> Take weight of pretrained model and adjust them to your own problem.\\\n",
    "\n",
    "A common workflow is to <b>freeze</b> all of the learned patterns in the bottom layers of a pretrained model so they're untrainable, then you can train and make adjustment of the outputs to your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50 V2 feature vector\n",
    "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "\n",
    "\n",
    "# EfficientNetB0 feature vector (version 2)\n",
    "efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_url, num_classes=10):\n",
    "    \"\"\"Take a TensorFlow Hub URL and create Keras Sequential model with it.\n",
    "\n",
    "    Returns:\n",
    "    uncompiled Keras Sequential model with model_url as feature\n",
    "    extractor layer and Dense output layer with num_classes outputs\n",
    "    \"\"\"\n",
    "    # Download pretrained model and save it as a Keras layer\n",
    "    feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "    trainable=False, # freeze the underlying patterns\n",
    "    name='feature_extraction_layer',\n",
    "    input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n",
    "\n",
    "    # Create our own model\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extractor_layer, # use feature extraction layer as the base\n",
    "        layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "resnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes)\n",
    "\n",
    "# Compile\n",
    "resnet_model.compile(loss='categorical_crossentropy',\n",
    "optimizer=tf.keras.optimizers.Adam(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20220514-223350\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 31s 889ms/step - loss: 1.9470 - accuracy: 0.3373 - val_loss: 1.1802 - val_accuracy: 0.6380\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 6s 276ms/step - loss: 0.9368 - accuracy: 0.7280 - val_loss: 0.8522 - val_accuracy: 0.7196\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 6s 263ms/step - loss: 0.6501 - accuracy: 0.8120 - val_loss: 0.7634 - val_accuracy: 0.7528\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 6s 253ms/step - loss: 0.5010 - accuracy: 0.8600 - val_loss: 0.7056 - val_accuracy: 0.7692\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 6s 254ms/step - loss: 0.3954 - accuracy: 0.9040 - val_loss: 0.6698 - val_accuracy: 0.7796\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "resnet_history = resnet_model.fit(train_data_10_percent,\n",
    "                                  epochs=5,\n",
    "                                  steps_per_epoch=len(train_data_10_percent),\n",
    "                                  validation_data=test_data,\n",
    "                                  validation_steps=len(test_data),\n",
    "                                  # Add TensorBoard callback to model (callbacks parameter takes a list)\n",
    "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", # save experiment logs here\n",
    "                                                                         experiment_name=\"resnet50V2\")]) # name of log files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we achieve 78% with only 5 epochs and 10% of training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "feature_extraction_layer (Ke (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 23,585,290\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means during training the model updates the 20,490 parameters in the ouput layer to suit our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorflow_hub/efficien/20220514-223501\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 29s 949ms/step - loss: 1.9577 - accuracy: 0.3653 - val_loss: 1.5268 - val_accuracy: 0.6132\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 7s 280ms/step - loss: 1.3072 - accuracy: 0.6960 - val_loss: 1.1335 - val_accuracy: 0.7032\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 6s 240ms/step - loss: 1.0056 - accuracy: 0.7587 - val_loss: 0.9549 - val_accuracy: 0.7420\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 6s 275ms/step - loss: 0.8397 - accuracy: 0.8080 - val_loss: 0.8619 - val_accuracy: 0.7548\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 6s 263ms/step - loss: 0.7336 - accuracy: 0.8293 - val_loss: 0.8043 - val_accuracy: 0.7640\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 6s 276ms/step - loss: 0.6564 - accuracy: 0.8467 - val_loss: 0.7627 - val_accuracy: 0.7724\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 7s 282ms/step - loss: 0.5908 - accuracy: 0.8707 - val_loss: 0.7330 - val_accuracy: 0.7800\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 7s 286ms/step - loss: 0.5373 - accuracy: 0.8907 - val_loss: 0.7094 - val_accuracy: 0.7824\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 6s 277ms/step - loss: 0.4936 - accuracy: 0.8973 - val_loss: 0.6906 - val_accuracy: 0.7840\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 7s 291ms/step - loss: 0.4532 - accuracy: 0.9160 - val_loss: 0.6778 - val_accuracy: 0.7872\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "efficientnet_model = create_model(model_url=efficientnet_url, # use EfficientNetB0 TensorFlow Hub URL\n",
    "                                  num_classes=train_data_10_percent.num_classes)\n",
    "\n",
    "# Compile EfficientNet model\n",
    "efficientnet_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=tf.keras.optimizers.Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Fit EfficientNet model \n",
    "efficientnet_history = efficientnet_model.fit(train_data_10_percent, # only use 10% of training data\n",
    "                                              epochs=10, # train for 10 epochs\n",
    "                                              steps_per_epoch=len(train_data_10_percent),\n",
    "                                              validation_data=test_data,\n",
    "                                              validation_steps=len(test_data),\n",
    "                                              callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", \n",
    "                                                                                     # Track logs under different experiment name\n",
    "                                                                                     experiment_name=\"efficien\")])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d76396febbd0a733882b1458c3ee4a5c8a662d67033c926e5be313f23d66902e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
