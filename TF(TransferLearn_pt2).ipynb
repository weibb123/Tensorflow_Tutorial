{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transfer Learning\n",
    "\n",
    "the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data\\\n",
    "\n",
    "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model(where the \"+\" indicates that many or all of the layers could be trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 15 18:30:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.13       Driver Version: 462.13       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 307... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   59C    P8    16W /  N/A |   1386MiB /  8192MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1720    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3428    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      6316    C+G   ...4\\UnrealCEFSubProcess.exe    N/A      |\n",
      "|    0   N/A  N/A     12936    C+G   ...LORANT-Win64-Shipping.exe    N/A      |\n",
      "|    0   N/A  N/A     19496    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Are we using a GPU? (if not & you're using Google Colab, go to Runtime -> Change Runtime Type -> Harware Accelerator: GPU )\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data (10% of labels)\n",
    "import zipfile\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test directories\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 files belonging to 10 classes.\n",
      "Found 2500 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224) # define image size\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
    "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
    "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
    "                                                                           image_size=IMG_SIZE,\n",
    "                                                                           label_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 224, 224, 3), (None, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the training data datatype\n",
    "train_data_10_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chicken_curry',\n",
       " 'chicken_wings',\n",
       " 'fried_rice',\n",
       " 'grilled_salmon',\n",
       " 'hamburger',\n",
       " 'ice_cream',\n",
       " 'pizza',\n",
       " 'ramen',\n",
       " 'steak',\n",
       " 'sushi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the class names of our dataset\n",
    "train_data_10_percent.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[193.11224    80.69898   126.0102   ]\n",
      "   [194.7602     84.94898   129.02551  ]\n",
      "   [189.63776    84.35714   124.86224  ]\n",
      "   ...\n",
      "   [181.38776    78.74496   116.94899  ]\n",
      "   [182.26534    80.31636   117.29084  ]\n",
      "   [180.7498     81.821236  115.923256 ]]\n",
      "\n",
      "  [[188.90816    82.71939   123.168365 ]\n",
      "   [191.84694    85.79082   125.77551  ]\n",
      "   [193.         86.78571   124.85714  ]\n",
      "   ...\n",
      "   [183.6889     79.21951   119.132774 ]\n",
      "   [180.70918    81.20412   118.994896 ]\n",
      "   [182.35704    87.97442   121.99482  ]]\n",
      "\n",
      "  [[186.42346    84.78061   117.85204  ]\n",
      "   [192.38776    86.53062   122.88776  ]\n",
      "   [195.95409    87.40306   124.21429  ]\n",
      "   ...\n",
      "   [174.37766    87.52551   126.1429   ]\n",
      "   [177.82654    86.57144   124.42856  ]\n",
      "   [174.87227    84.657974  120.00997  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[206.92346   229.92346   243.92346  ]\n",
      "   [209.92857   231.92857   245.92857  ]\n",
      "   [211.        233.        247.       ]\n",
      "   ...\n",
      "   [113.806274  126.23486   126.44916  ]\n",
      "   [140.38284   152.81143   152.81143  ]\n",
      "   [174.57666   188.57666   189.36237  ]]\n",
      "\n",
      "  [[206.64285   229.64285   243.64285  ]\n",
      "   [209.92857   231.92857   245.92857  ]\n",
      "   [211.        233.        247.       ]\n",
      "   ...\n",
      "   [115.39283   129.39284   130.79076  ]\n",
      "   [123.933754  137.93375   137.93375  ]\n",
      "   [167.15848   181.15848   181.15848  ]]\n",
      "\n",
      "  [[206.87245   229.87245   243.87245  ]\n",
      "   [209.33163   231.33163   245.33163  ]\n",
      "   [210.        232.        246.       ]\n",
      "   ...\n",
      "   [120.55592   134.77019   137.20894  ]\n",
      "   [119.48989   133.48988   133.48988  ]\n",
      "   [151.54132   163.54132   163.54132  ]]]\n",
      "\n",
      "\n",
      " [[[180.93878   138.93878   100.938774 ]\n",
      "   [183.2296    139.2296    102.22959  ]\n",
      "   [184.91325   138.91325   104.91327  ]\n",
      "   ...\n",
      "   [201.40291   181.12743   146.26013  ]\n",
      "   [178.23976   163.43367   131.16837  ]\n",
      "   [190.55644   180.2708    150.62791  ]]\n",
      "\n",
      "  [[185.2398    141.2398    104.23979  ]\n",
      "   [187.44388   143.44388   108.44388  ]\n",
      "   [193.11736   146.47449   113.68878  ]\n",
      "   ...\n",
      "   [206.87225   186.44373   149.15805  ]\n",
      "   [182.79582   164.21422   131.22443  ]\n",
      "   [177.05614   162.25002   129.32144  ]]\n",
      "\n",
      "  [[186.04591   141.83163   107.26021  ]\n",
      "   [183.30612   137.12245   104.75     ]\n",
      "   [184.18367   135.09183   105.22959  ]\n",
      "   ...\n",
      "   [226.69894   204.79077   167.11732  ]\n",
      "   [213.3162    191.42845   153.99988  ]\n",
      "   [188.14752   167.71895   131.57602  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 82.23987    79.08682    71.658165 ]\n",
      "   [ 56.76011    60.918335   42.601883 ]\n",
      "   [ 47.70928    56.877693   36.73477  ]\n",
      "   ...\n",
      "   [175.23997   123.882774   84.19407  ]\n",
      "   [169.11731   118.561134   75.40295  ]\n",
      "   [168.64261   119.2804     71.06099  ]]\n",
      "\n",
      "  [[ 80.55615    78.74492    59.459137 ]\n",
      "   [ 61.27048    65.05108    37.979595 ]\n",
      "   [ 41.8725     52.214325   26.142872 ]\n",
      "   ...\n",
      "   [169.92856   113.92856    78.50003  ]\n",
      "   [169.29597   113.50009    76.423546 ]\n",
      "   [166.5253    112.81104    68.948814 ]]\n",
      "\n",
      "  [[ 81.60695    76.122246   47.70893  ]\n",
      "   [ 69.4593     70.59708    35.765343 ]\n",
      "   [ 46.116997   53.101665   23.89249  ]\n",
      "   ...\n",
      "   [172.78078   114.42356    80.342064 ]\n",
      "   [176.18881   117.26025    83.04592  ]\n",
      "   [178.11209   120.11208    81.39773  ]]]\n",
      "\n",
      "\n",
      " [[[ 34.642857   55.642857   72.64286  ]\n",
      "   [ 34.142857   53.142857   70.14285  ]\n",
      "   [ 32.290817   49.357143   64.581635 ]\n",
      "   ...\n",
      "   [ 14.005095   14.005095    6.0050945]\n",
      "   [ 12.908159   12.908159    4.9081597]\n",
      "   [ 12.         12.          4.       ]]\n",
      "\n",
      "  [[ 33.785713   52.785713   69.78572  ]\n",
      "   [ 34.010204   53.010204   68.0102   ]\n",
      "   [ 35.07143    52.285717   67.85715  ]\n",
      "   ...\n",
      "   [ 15.872469   15.872469    7.872469 ]\n",
      "   [ 14.857117   14.857117    6.8571167]\n",
      "   [ 11.311157   11.311157    3.3111572]]\n",
      "\n",
      "  [[ 36.642857   55.42857    70.21429  ]\n",
      "   [ 35.341835   54.12755    68.91326  ]\n",
      "   [ 34.045918   51.64286    66.92347  ]\n",
      "   ...\n",
      "   [ 15.83165    15.83165     7.83165  ]\n",
      "   [ 15.770405   15.770405    7.7704053]\n",
      "   [ 14.5050745  14.5050745   6.5050745]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   ...\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]]\n",
      "\n",
      "  [[  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   ...\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]]\n",
      "\n",
      "  [[  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   ...\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]\n",
      "   [  1.          1.          1.       ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[  3.          4.          6.       ]\n",
      "   [  3.97449     4.9744897   6.9744897]\n",
      "   [  4.719388    6.719388    5.2908163]\n",
      "   ...\n",
      "   [118.10206    98.66833    88.89282  ]\n",
      "   [119.07126    91.97941    83.02533  ]\n",
      "   [ 89.87192    59.871914   51.871914 ]]\n",
      "\n",
      "  [[  4.          5.          7.       ]\n",
      "   [  4.          5.          7.       ]\n",
      "   [  5.          7.          5.5714283]\n",
      "   ...\n",
      "   [ 53.148224   36.132812   31.520607 ]\n",
      "   [132.883     104.73499   100.46457  ]\n",
      "   [175.48482   141.82149   136.34697  ]]\n",
      "\n",
      "  [[  6.4948983   6.4948983   8.494898 ]\n",
      "   [  6.015306    6.015306    8.015306 ]\n",
      "   [  6.          6.          5.5714283]\n",
      "   ...\n",
      "   [ 29.290764   16.836592   17.770258 ]\n",
      "   [ 47.52563    24.540888   24.357182 ]\n",
      "   [ 91.465065   62.3936     59.959885 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 17.24975    12.821222   13.606958 ]\n",
      "   [ 49.0455     43.157753   43.213875 ]\n",
      "   [ 53.31153    43.74524    44.362625 ]\n",
      "   ...\n",
      "   [ 39.97459     9.974589    7.9745893]\n",
      "   [ 43.51534    14.301074   10.729602 ]\n",
      "   [ 39.285645   10.285645    6.2856445]]\n",
      "\n",
      "  [[  7.04594     5.04594     8.04594  ]\n",
      "   [ 13.382593   11.382593   12.658128 ]\n",
      "   [ 54.851845   48.96411    50.907978 ]\n",
      "   ...\n",
      "   [ 41.484722   12.270459    8.698987 ]\n",
      "   [ 42.933662   13.933662    9.933662 ]\n",
      "   [ 39.90818    10.908182    6.908182 ]]\n",
      "\n",
      "  [[  9.673305    4.673305   10.673305 ]\n",
      "   [  6.847098    4.847098    8.653285 ]\n",
      "   [ 19.668058   17.239487   19.810915 ]\n",
      "   ...\n",
      "   [ 43.796066   14.796066   10.796066 ]\n",
      "   [ 42.617306   13.617305    9.617305 ]\n",
      "   [ 43.770397   14.770398   10.770398 ]]]\n",
      "\n",
      "\n",
      " [[[114.85204   103.92347    88.28062  ]\n",
      "   [108.20918    99.87245    84.54082  ]\n",
      "   [111.872444  103.36735    89.35714  ]\n",
      "   ...\n",
      "   [  6.428528    8.          7.214264 ]\n",
      "   [  6.          8.          7.       ]\n",
      "   [  7.          9.          8.       ]]\n",
      "\n",
      "  [[108.18878   100.42347    87.4949   ]\n",
      "   [109.65306   103.66326    89.94388  ]\n",
      "   [110.408165  104.551025   92.65307  ]\n",
      "   ...\n",
      "   [  7.214264    8.785736    8.       ]\n",
      "   [  6.1377673   8.137767    7.1377673]\n",
      "   [  7.          9.          8.       ]]\n",
      "\n",
      "  [[108.841835  105.69898    93.97959  ]\n",
      "   [124.85204   121.63776   112.20918  ]\n",
      "   [186.09692   182.88264   173.88264  ]\n",
      "   ...\n",
      "   [  7.831628    8.168372    8.       ]\n",
      "   [  7.          9.          8.       ]\n",
      "   [  7.          9.          8.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[113.08684   106.08684    90.08684  ]\n",
      "   [116.775536  109.775536   93.775536 ]\n",
      "   [113.74993   106.74993    90.74993  ]\n",
      "   ...\n",
      "   [ 82.525314   78.525314   66.525314 ]\n",
      "   [ 82.14802    78.14802    66.14802  ]\n",
      "   [ 81.12254    77.12254    66.12254  ]]\n",
      "\n",
      "  [[111.42836   104.42836    88.42836  ]\n",
      "   [110.86719   103.86719    87.86719  ]\n",
      "   [102.48459    95.48459    79.48459  ]\n",
      "   ...\n",
      "   [ 74.545845   70.545845   58.54584  ]\n",
      "   [ 91.92874    87.92874    75.92874  ]\n",
      "   [ 88.0764     84.0764     73.0764   ]]\n",
      "\n",
      "  [[ 87.071465   80.071465   64.071465 ]\n",
      "   [ 92.122475   85.122475   69.122475 ]\n",
      "   [ 97.27041    90.27041    74.27041  ]\n",
      "   ...\n",
      "   [ 78.622345   74.622345   62.62234  ]\n",
      "   [ 82.50009    78.50009    66.50009  ]\n",
      "   [ 86.86233    82.86233    71.86233  ]]]\n",
      "\n",
      "\n",
      " [[[164.60715   133.60715    76.60714  ]\n",
      "   [162.93367   131.93367    76.841835 ]\n",
      "   [159.88776   126.168365   76.04082  ]\n",
      "   ...\n",
      "   [ 84.21437    21.214373   12.214373 ]\n",
      "   [ 83.5306     21.739813   12.336744 ]\n",
      "   [ 85.74494    25.744944   15.744943 ]]\n",
      "\n",
      "  [[169.94388   139.94388    77.42347  ]\n",
      "   [167.7653    137.69897    77.765305 ]\n",
      "   [161.63774   127.994896   74.5102   ]\n",
      "   ...\n",
      "   [ 84.413315   20.413311   11.413311 ]\n",
      "   [ 90.306046   26.443815   17.443815 ]\n",
      "   [ 89.54616    25.949253   16.949253 ]]\n",
      "\n",
      "  [[167.36224   138.14795    69.71939  ]\n",
      "   [165.15816   134.15816    69.85714  ]\n",
      "   [166.19388   133.55103    75.19388  ]\n",
      "   ...\n",
      "   [ 90.59697    23.168394   15.38268  ]\n",
      "   [ 96.831604   29.40303    21.617315 ]\n",
      "   [ 94.97967    27.551094   19.76538  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[242.56633   218.56633   182.99486  ]\n",
      "   [240.15816   216.15816   182.04591  ]\n",
      "   [239.78572   214.78572   183.95409  ]\n",
      "   ...\n",
      "   [203.29596   148.7756     74.607254 ]\n",
      "   [189.51527   139.65816    69.44389  ]\n",
      "   [216.13792   171.2808    109.85224  ]]\n",
      "\n",
      "  [[241.26529   217.26529   179.26529  ]\n",
      "   [239.92346   215.92346   179.92346  ]\n",
      "   [238.0153    213.0153    182.22958  ]\n",
      "   ...\n",
      "   [188.41856   142.82162    69.418434 ]\n",
      "   [186.40823   136.54091    63.47457  ]\n",
      "   [202.81104   153.7396     86.928375 ]]\n",
      "\n",
      "  [[238.71425   215.71425   174.71425  ]\n",
      "   [238.83163   214.83163   178.68878  ]\n",
      "   [238.29082   213.21428   182.65819  ]\n",
      "   ...\n",
      "   [177.23979   136.52042    65.7448   ]\n",
      "   [187.16325   139.09181    63.09182  ]\n",
      "   [186.58183   133.22466    62.765457 ]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# see an example batch of data\n",
    "for images, labels in train_data_10_percent.take(1):\n",
    "    print(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Building a transfer learning model using the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 2s 0us/step\n",
      "16719872/16705208 [==============================] - 2s 0us/step\n",
      "Shape after base_model: (None, 7, 7, 1280)\n",
      "After GlobalAveragePooling2D(): (None, 1280)\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 15s 200ms/step - loss: 1.9126 - accuracy: 0.4093 - val_loss: 1.3469 - val_accuracy: 0.7220\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 1.1513 - accuracy: 0.7440 - val_loss: 0.9431 - val_accuracy: 0.7878\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 3s 106ms/step - loss: 0.8521 - accuracy: 0.7987 - val_loss: 0.7580 - val_accuracy: 0.8109\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 3s 104ms/step - loss: 0.6973 - accuracy: 0.8400 - val_loss: 0.6788 - val_accuracy: 0.8322\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 3s 108ms/step - loss: 0.5961 - accuracy: 0.8587 - val_loss: 0.6363 - val_accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "# 1. Create base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# 2. Freeze the base model (so the pre-learned patterns remain)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Create inputs into the base model\n",
    "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "\n",
    "# 4. Pass the inputs to the base_model\n",
    "x = base_model(inputs)\n",
    "# check data shape after passing it to base_model\n",
    "print(f\"Shape after base_model: {x.shape}\")\n",
    "\n",
    "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n",
    "\n",
    "# 7. Create the output activation layer\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# 8. Combine the inputs with the outputs into a model\n",
    "model_0 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 9. Compile the model\n",
    "model_0.compile(loss='categorical_crossentropy',\n",
    "optimizer=tf.keras.optimizers.Adam(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# 10. Fit the model (we use less steps for validation so it's faster)\n",
    "history_10_percent = model_0.fit(train_data_10_percent,\n",
    "epochs=5,\n",
    "steps_per_epoch=len(train_data_10_percent),\n",
    "validation_data=test_data_10_percent,\n",
    "# Go through less of the validation data so epochs are faster (we want faster experiments!)\n",
    "validation_steps=int(0.25 * len(test_data_10_percent)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is called feature extraction transfer learning, similar to what we did with the TensorFlow Hub models.\n",
    "\n",
    "In other words, we passed our custom data to a pre-trained model, and then put our own output layer on top to make sure the outputs were tailored to our desired number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 rescaling\n",
      "2 normalization\n",
      "3 stem_conv_pad\n",
      "4 stem_conv\n",
      "5 stem_bn\n",
      "6 stem_activation\n",
      "7 block1a_dwconv\n",
      "8 block1a_bn\n",
      "9 block1a_activation\n",
      "10 block1a_se_squeeze\n",
      "11 block1a_se_reshape\n",
      "12 block1a_se_reduce\n",
      "13 block1a_se_expand\n",
      "14 block1a_se_excite\n",
      "15 block1a_project_conv\n",
      "16 block1a_project_bn\n",
      "17 block2a_expand_conv\n",
      "18 block2a_expand_bn\n",
      "19 block2a_expand_activation\n",
      "20 block2a_dwconv_pad\n",
      "21 block2a_dwconv\n",
      "22 block2a_bn\n",
      "23 block2a_activation\n",
      "24 block2a_se_squeeze\n",
      "25 block2a_se_reshape\n",
      "26 block2a_se_reduce\n",
      "27 block2a_se_expand\n",
      "28 block2a_se_excite\n",
      "29 block2a_project_conv\n",
      "30 block2a_project_bn\n",
      "31 block2b_expand_conv\n",
      "32 block2b_expand_bn\n",
      "33 block2b_expand_activation\n",
      "34 block2b_dwconv\n",
      "35 block2b_bn\n",
      "36 block2b_activation\n",
      "37 block2b_se_squeeze\n",
      "38 block2b_se_reshape\n",
      "39 block2b_se_reduce\n",
      "40 block2b_se_expand\n",
      "41 block2b_se_excite\n",
      "42 block2b_project_conv\n",
      "43 block2b_project_bn\n",
      "44 block2b_drop\n",
      "45 block2b_add\n",
      "46 block3a_expand_conv\n",
      "47 block3a_expand_bn\n",
      "48 block3a_expand_activation\n",
      "49 block3a_dwconv_pad\n",
      "50 block3a_dwconv\n",
      "51 block3a_bn\n",
      "52 block3a_activation\n",
      "53 block3a_se_squeeze\n",
      "54 block3a_se_reshape\n",
      "55 block3a_se_reduce\n",
      "56 block3a_se_expand\n",
      "57 block3a_se_excite\n",
      "58 block3a_project_conv\n",
      "59 block3a_project_bn\n",
      "60 block3b_expand_conv\n",
      "61 block3b_expand_bn\n",
      "62 block3b_expand_activation\n",
      "63 block3b_dwconv\n",
      "64 block3b_bn\n",
      "65 block3b_activation\n",
      "66 block3b_se_squeeze\n",
      "67 block3b_se_reshape\n",
      "68 block3b_se_reduce\n",
      "69 block3b_se_expand\n",
      "70 block3b_se_excite\n",
      "71 block3b_project_conv\n",
      "72 block3b_project_bn\n",
      "73 block3b_drop\n",
      "74 block3b_add\n",
      "75 block4a_expand_conv\n",
      "76 block4a_expand_bn\n",
      "77 block4a_expand_activation\n",
      "78 block4a_dwconv_pad\n",
      "79 block4a_dwconv\n",
      "80 block4a_bn\n",
      "81 block4a_activation\n",
      "82 block4a_se_squeeze\n",
      "83 block4a_se_reshape\n",
      "84 block4a_se_reduce\n",
      "85 block4a_se_expand\n",
      "86 block4a_se_excite\n",
      "87 block4a_project_conv\n",
      "88 block4a_project_bn\n",
      "89 block4b_expand_conv\n",
      "90 block4b_expand_bn\n",
      "91 block4b_expand_activation\n",
      "92 block4b_dwconv\n",
      "93 block4b_bn\n",
      "94 block4b_activation\n",
      "95 block4b_se_squeeze\n",
      "96 block4b_se_reshape\n",
      "97 block4b_se_reduce\n",
      "98 block4b_se_expand\n",
      "99 block4b_se_excite\n",
      "100 block4b_project_conv\n",
      "101 block4b_project_bn\n",
      "102 block4b_drop\n",
      "103 block4b_add\n",
      "104 block4c_expand_conv\n",
      "105 block4c_expand_bn\n",
      "106 block4c_expand_activation\n",
      "107 block4c_dwconv\n",
      "108 block4c_bn\n",
      "109 block4c_activation\n",
      "110 block4c_se_squeeze\n",
      "111 block4c_se_reshape\n",
      "112 block4c_se_reduce\n",
      "113 block4c_se_expand\n",
      "114 block4c_se_excite\n",
      "115 block4c_project_conv\n",
      "116 block4c_project_bn\n",
      "117 block4c_drop\n",
      "118 block4c_add\n",
      "119 block5a_expand_conv\n",
      "120 block5a_expand_bn\n",
      "121 block5a_expand_activation\n",
      "122 block5a_dwconv\n",
      "123 block5a_bn\n",
      "124 block5a_activation\n",
      "125 block5a_se_squeeze\n",
      "126 block5a_se_reshape\n",
      "127 block5a_se_reduce\n",
      "128 block5a_se_expand\n",
      "129 block5a_se_excite\n",
      "130 block5a_project_conv\n",
      "131 block5a_project_bn\n",
      "132 block5b_expand_conv\n",
      "133 block5b_expand_bn\n",
      "134 block5b_expand_activation\n",
      "135 block5b_dwconv\n",
      "136 block5b_bn\n",
      "137 block5b_activation\n",
      "138 block5b_se_squeeze\n",
      "139 block5b_se_reshape\n",
      "140 block5b_se_reduce\n",
      "141 block5b_se_expand\n",
      "142 block5b_se_excite\n",
      "143 block5b_project_conv\n",
      "144 block5b_project_bn\n",
      "145 block5b_drop\n",
      "146 block5b_add\n",
      "147 block5c_expand_conv\n",
      "148 block5c_expand_bn\n",
      "149 block5c_expand_activation\n",
      "150 block5c_dwconv\n",
      "151 block5c_bn\n",
      "152 block5c_activation\n",
      "153 block5c_se_squeeze\n",
      "154 block5c_se_reshape\n",
      "155 block5c_se_reduce\n",
      "156 block5c_se_expand\n",
      "157 block5c_se_excite\n",
      "158 block5c_project_conv\n",
      "159 block5c_project_bn\n",
      "160 block5c_drop\n",
      "161 block5c_add\n",
      "162 block6a_expand_conv\n",
      "163 block6a_expand_bn\n",
      "164 block6a_expand_activation\n",
      "165 block6a_dwconv_pad\n",
      "166 block6a_dwconv\n",
      "167 block6a_bn\n",
      "168 block6a_activation\n",
      "169 block6a_se_squeeze\n",
      "170 block6a_se_reshape\n",
      "171 block6a_se_reduce\n",
      "172 block6a_se_expand\n",
      "173 block6a_se_excite\n",
      "174 block6a_project_conv\n",
      "175 block6a_project_bn\n",
      "176 block6b_expand_conv\n",
      "177 block6b_expand_bn\n",
      "178 block6b_expand_activation\n",
      "179 block6b_dwconv\n",
      "180 block6b_bn\n",
      "181 block6b_activation\n",
      "182 block6b_se_squeeze\n",
      "183 block6b_se_reshape\n",
      "184 block6b_se_reduce\n",
      "185 block6b_se_expand\n",
      "186 block6b_se_excite\n",
      "187 block6b_project_conv\n",
      "188 block6b_project_bn\n",
      "189 block6b_drop\n",
      "190 block6b_add\n",
      "191 block6c_expand_conv\n",
      "192 block6c_expand_bn\n",
      "193 block6c_expand_activation\n",
      "194 block6c_dwconv\n",
      "195 block6c_bn\n",
      "196 block6c_activation\n",
      "197 block6c_se_squeeze\n",
      "198 block6c_se_reshape\n",
      "199 block6c_se_reduce\n",
      "200 block6c_se_expand\n",
      "201 block6c_se_excite\n",
      "202 block6c_project_conv\n",
      "203 block6c_project_bn\n",
      "204 block6c_drop\n",
      "205 block6c_add\n",
      "206 block6d_expand_conv\n",
      "207 block6d_expand_bn\n",
      "208 block6d_expand_activation\n",
      "209 block6d_dwconv\n",
      "210 block6d_bn\n",
      "211 block6d_activation\n",
      "212 block6d_se_squeeze\n",
      "213 block6d_se_reshape\n",
      "214 block6d_se_reduce\n",
      "215 block6d_se_expand\n",
      "216 block6d_se_excite\n",
      "217 block6d_project_conv\n",
      "218 block6d_project_bn\n",
      "219 block6d_drop\n",
      "220 block6d_add\n",
      "221 block7a_expand_conv\n",
      "222 block7a_expand_bn\n",
      "223 block7a_expand_activation\n",
      "224 block7a_dwconv\n",
      "225 block7a_bn\n",
      "226 block7a_activation\n",
      "227 block7a_se_squeeze\n",
      "228 block7a_se_reshape\n",
      "229 block7a_se_reduce\n",
      "230 block7a_se_expand\n",
      "231 block7a_se_excite\n",
      "232 block7a_project_conv\n",
      "233 block7a_project_bn\n",
      "234 top_conv\n",
      "235 top_bn\n",
      "236 top_activation\n"
     ]
    }
   ],
   "source": [
    "# check layers in our base model\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "    print(layer_number, layer.name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d76396febbd0a733882b1458c3ee4a5c8a662d67033c926e5be313f23d66902e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
